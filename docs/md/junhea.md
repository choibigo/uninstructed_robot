## 준희 일정

#### Todo List
- [ ] Anaconda => VSCODE => Python 예제 실행해보기
- [X] python - 프로그래머스 Lv0 끝에 10개 풀어보기
- [X] Github
- [ ] 밑바닥 부터 시작하는 딥러닝 (2)
- [ ] Pytorch 공부 <모두를 위한 딥러닝>

1번 vscode에서 import할 다른 패키지 다운로드 및 파일위치 지정방법 알아보기(질문 예정)

2번 시간만 충분하다면 전부 풀 수 있는 것 같습니다. 한문제당 짧게는 5분안에, 길게는 1시간 30분정도 걸렸습니다. 그래도 전부 정답을 맞추긴 했습니다.

3번 vscode와 깃허브 연동은 알아보았지만 아직 진행하진 않았습니다.

4번 1권이 거의 끝나가지만 1~3번을 수행하느라 진도가 아직 나가지 않았습니다. 못해도 이번 설날이 오기 전에 시작할 예정입니다. (2월 8일 시작 ~ )

5번 -


#### 질문
1) 컨볼루션 연산은 정말 컨볼루션연산일까? 이상한점 1개 생각해보기

x값 (데이터 합성곱에서는 '필터'의 시작 지점 / 이하 x로 표현 통일)이 마치 델타 함수처럼 양자화 되어 있다. 하지만 이것만으로는 차이를 논하기에는 부족하니 몇가지 더 찾아보면 x의 차원(dimm)이 다르다. 이로 인해 결과값이 일반적인 합성곱에서 f * g 에서 g의 범위가 커질수록 결과의 범위가 커졌던 것과 달리 '필터'가 커질수록 결과의 범위(행렬의 크기)가 작아진다. 따라서 이를 막기 위해서 입력 데이터에 '패딩'처리를 하거나 스트라이드를 이용하여 결과를 임의로 조정해야 한다. 즉 '입력 데이터' (= f(x)) 와 '필터' (=g(x)) 가 변하지 않아도 결과값이 바뀔 수 있으며 일반적인 합성곱 연산과 달리 답이 여러개인 상황이 생긴다.

2) L1-Regularization, L2-Regularization의 차이는 무엇일까?(수학적 차이 말고 모델이 미치는 영향)

훈련 데이터가 적을 때 오버피딩을 예방하기 위해 '가중치 감소'방법이 있는데 이는 norm(노름)을 손실함수(음수) 에 더하여 손실함수의 크기가 작아지게 해 큰 가중치에 대해 큰 패널티를 작은 가중치에 대해 작은 패널티를 부여하는 방법이다. 또한 이 방식은 가중치의 모든 원소를 0에 가깝게 하여 기울기를 작게 만들어서 오버피딩이 일어나지 않게 모데를 강제로 제한하기 위해 쓰이는데 여기에 쓰이는 규제 방법에는 여러가지가 있다. 그중 대표적인 2개를 선택하면 L1과 L2가 있는데 L1은 각 가중치의 절댓값의 합이며 L2는 각 가중치의 제곱의 합에 루트를 씌운 형태이다. 앞서서 모든 원소를 0에 가깝게 하여 기울기를 작게 한다고 했는데 L1방식을 사용하면 몇몇 작은 가중치들은 실제로 0이 될 수 있어서 모델에서 완전히 제외되는 특성이 생길 수 있고 해당 모델에서 중요하게 생각하는 특징을 쉽게 알 수 있지만 안정적이지 않다. 하지만 L2 방식은 모델에서 제외되는 특성이 없으므로 L1보다는 조금 더 안정적인 방식이고, 가중치 각각의 영향력이 작을 때 쓰는 것이 좋다.

3) self-supervised learning이란 무엇일까?, 이 내용에서 나오는 Downstream이란 용어의 의미는 무엇일까?

self-supervised learning이란 엄청나게 많은 양의 데이터를 스스로 라벨을 부여해 학습하는 모델을 의미합니다. 예를 들어 그림을 회전시켜서 각도라는 라벨을 부여하여 데이터로부터 곧바로 얻을 수 있는 종류의 라벨을 부여하여 학습을 진행하게 됩니다. 이때 Downstream task는 앞에서 self-supervised learning으로 학습한 모델을 transfer하여 사용하며 라벨이 있는 소규모 데이터를 이용하여 downstream task에 맞게 추가된 계층만 학습하게 됩니다.


https://www.youtube.com/watch?v=MxKqIR5Rffc  (vscode와 깃허브 연동 시도중...)
https://shortcuts.tistory.com/8              (git의 각종 명령어들)
