## 호성 일정

#### Todo List
- [X] PEP 8
- [X] MD 파일 작성법
- [X] 밑바닥 부터 시작하는 딥러닝 (1)
- [ ] 밑바닥 부터 시작하는 딥러닝 (2)
- [ ] Git 기본 사용법 (VScode로 해보는거 추천)
- [ ] Pytorch 공부 (모두를 위한 딥러닝 추천)
- [ ] 토이 프로젝트 (주제는 함께 생각해 볼것)


#### 일정 관리

기간 | 내용 | 질문
-- | -- | -- 
11/29 ~ 12/06  | - PEP8 </br> - MD 작성법 |
12/07 ~ 12/13  | - 진행사항 없음(시험 기간 ~ 12월 20일) |
12/14 ~ 12/20  | - 진행사항 없음(시험 기간 ~ 12월 20일) |
12/21 ~ 12/27  | - 밑바닥부터 시작하는 딥러닝1 완료 | - 컨볼루션 연산은 정말 컨볼루션연산일까? 이상한점 1개 생각해보기 </br> - l1-Regularization, l2-Regularization의 차이는 무엇일까?(수학적 차이 말고 모델이 미치는 영향) </br> - self-supervised learning이란 무엇일까?, 이 내용에서 나오는 Downstream이란 용어의 의미는 무엇일까? | 
12/28 ~ 1/03   | - 밑바닥부터 시작하는 딥러닝2 3.1 | 


#### 밑바닥부터 시작하는 딥러닝1 질문
##### 1. 컨볼루션 연산은 정말 컨볼루션연산일까? 이상한점 1개 생각해보기
뒤집는 단계가 없는 연산?   
데이터의 처리과정에서 사용하는 컨볼루션은 정확히 말하면 상호 상관도(cross-correlation) 연산의 일종이로 커널이 이동하는 범위가 데이터의 크기로 고정되어 있는 것이다. 
##### 2. L1-Regularization, L2-Regularization의 차이는 무엇일까?(수학적 차이 말고 모델이 미치는 영향)
l1-norm : $|w_1|+|w_2| + \cdots +|w_n|$ → 맨하탄 거리 : 여러 방법이 존재하는 거리   
l2-norm : $\sqrt{w_1^2 + w_x^2 +\cdots+w_n^2}$ → 유클리드 : 최단거리   
l1-regulation을 사용하는 경우, 특정 가중치를 제거하여 원하는 가중치들만 남기는 feature selection이 가능하다. 이를 통해 모델의 복잡도를 낮출 수 있어진다.   
l2-regulation의 경우에는 가중치의 크기에 따라 가중치 값의 변화가 이루어지는 방식이다.(weight decay 에서 l2-norm 사용) 해당 방식을 사용하는 경우에는 가중치의 크기에 따라 가중치 패널티의 정도가 결정되므로 가중치가 전체적으로 작아지게 된고 이를 통해 학습 효과가 l1보다 좋게 나타나기도 한다.   
##### 3. Self-supervised learning이란 무엇일까?, 이 내용에서 나오는 Downstream이란 용어의 의미는 무엇일까?
자기지도 학습 - 지도학습과 비지도학습의 섞인 형태. 데이터는 비지도학습과 같이 레이블이 없는 데이터를 학습 데이터로 입력받아 스스로 데이터의 분류 과정을 거쳐 특징들을 찾고 찾은 특징들에 맞춰 다시 학습을 진행한다.    
위 과정은 즉, 두개의 단계로 나뉘는데 pre-traing 혹은 pretext task가 첫번째 단계, fine tuning 또는 downstream task가 두번째 단계이다.    
pretext 단계에서는 다수의 레이블이 없는 데이터를 입력받아 특징(feature)들을 찾는 단계이고   
downstream task 단계에서는 앞서 찾은 특징들에 맞춰 설정된 학습 모델에 소량의 데이터를 입력하여 보다 목적성이 뚜렷하고 정확한 모델로 학습시키는 단계이다.   
위의 downstream이라는 용어는 최종적인 학습의 목표 또는 이유 같은 것으로 볼 수 있는듯 하다.   
